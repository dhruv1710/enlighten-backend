{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-04T07:10:41.591053Z","iopub.execute_input":"2025-03-04T07:10:41.591330Z","iopub.status.idle":"2025-03-04T07:10:41.596257Z","shell.execute_reply.started":"2025-03-04T07:10:41.591304Z","shell.execute_reply":"2025-03-04T07:10:41.595438Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"!pip install -U wandb peft transformers huggingface_hub trl bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T07:10:41.597106Z","iopub.execute_input":"2025-03-04T07:10:41.597341Z","iopub.status.idle":"2025-03-04T07:11:10.493915Z","shell.execute_reply.started":"2025-03-04T07:10:41.597323Z","shell.execute_reply":"2025-03-04T07:11:10.492856Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.1)\nCollecting wandb\n  Downloading wandb-0.19.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nCollecting transformers\n  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.29.0)\nCollecting huggingface_hub\n  Downloading huggingface_hub-0.29.1-py3-none-any.whl.metadata (13 kB)\nCollecting trl\n  Downloading trl-0.15.2-py3-none-any.whl.metadata (11 kB)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\nRequirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.11.0a2)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\nRequirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.2)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.5.1+cu121)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.67.1)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.2.1)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.5)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.12.0)\nRequirement already satisfied: datasets>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from trl) (3.3.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl) (13.9.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (0.70.16)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (3.11.12)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.29.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (2.19.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.18.3)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->peft) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->peft) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.21.0->trl) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.21.0->trl) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.21.0->trl) (2025.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->peft) (2024.2.0)\nDownloading wandb-0.19.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.29.1-py3-none-any.whl (468 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.0/468.0 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading trl-0.15.2-py3-none-any.whl (318 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: huggingface_hub, wandb, transformers, trl, bitsandbytes\n  Attempting uninstall: huggingface_hub\n    Found existing installation: huggingface-hub 0.29.0\n    Uninstalling huggingface-hub-0.29.0:\n      Successfully uninstalled huggingface-hub-0.29.0\n  Attempting uninstall: wandb\n    Found existing installation: wandb 0.19.1\n    Uninstalling wandb-0.19.1:\n      Successfully uninstalled wandb-0.19.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.47.0\n    Uninstalling transformers-4.47.0:\n      Successfully uninstalled transformers-4.47.0\nSuccessfully installed bitsandbytes-0.45.3 huggingface_hub-0.29.1 transformers-4.49.0 trl-0.15.2 wandb-0.19.7\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nimport wandb\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n\nlogin(token = hf_token)\n\nwb_token = user_secrets.get_secret(\"WANDB_TOKEN\")\n\nwandb.login(key=wb_token)\nrun = wandb.init(\n    project='Fine-tune Llama 3 8B on Socratic Chat Dataset', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T07:11:10.495125Z","iopub.execute_input":"2025-03-04T07:11:10.495453Z","iopub.status.idle":"2025-03-04T07:11:24.945706Z","shell.execute_reply.started":"2025-03-04T07:11:10.495422Z","shell.execute_reply":"2025-03-04T07:11:24.944989Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdhruv1710\u001b[0m (\u001b[33mdhruv1710-kartavya-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250304_071118-2122q580</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/dhruv1710-kartavya-technology/Fine-tune%20Llama%203%208B%20on%20Socratic%20Chat%20Dataset/runs/2122q580?apiKey=73e243c9b1927f03120b4cc2517d5699f2db0893' target=\"_blank\">morning-meadow-3</a></strong> to <a href='https://wandb.ai/dhruv1710-kartavya-technology/Fine-tune%20Llama%203%208B%20on%20Socratic%20Chat%20Dataset?apiKey=73e243c9b1927f03120b4cc2517d5699f2db0893' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/dhruv1710-kartavya-technology/Fine-tune%20Llama%203%208B%20on%20Socratic%20Chat%20Dataset?apiKey=73e243c9b1927f03120b4cc2517d5699f2db0893' target=\"_blank\">https://wandb.ai/dhruv1710-kartavya-technology/Fine-tune%20Llama%203%208B%20on%20Socratic%20Chat%20Dataset?apiKey=73e243c9b1927f03120b4cc2517d5699f2db0893</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/dhruv1710-kartavya-technology/Fine-tune%20Llama%203%208B%20on%20Socratic%20Chat%20Dataset/runs/2122q580?apiKey=73e243c9b1927f03120b4cc2517d5699f2db0893' target=\"_blank\">https://wandb.ai/dhruv1710-kartavya-technology/Fine-tune%20Llama%203%208B%20on%20Socratic%20Chat%20Dataset/runs/2122q580?apiKey=73e243c9b1927f03120b4cc2517d5699f2db0893</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Do NOT share these links with anyone. They can be used to claim your runs."},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset('FreedomIntelligence/SocraticChat',split='train[0:500]')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T07:11:24.947488Z","iopub.execute_input":"2025-03-04T07:11:24.947694Z","iopub.status.idle":"2025-03-04T07:11:26.199496Z","shell.execute_reply.started":"2025-03-04T07:11:24.947676Z","shell.execute_reply":"2025-03-04T07:11:26.198630Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"dataset[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T07:11:26.200571Z","iopub.execute_input":"2025-03-04T07:11:26.200838Z","iopub.status.idle":"2025-03-04T07:11:26.207383Z","shell.execute_reply.started":"2025-03-04T07:11:26.200817Z","shell.execute_reply":"2025-03-04T07:11:26.206278Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'conversations': [{'from': 'human',\n   'value': 'You are a psychologist that specializes in CBT therapy. You are also a Christian. You have been asked to create a Bible study that teaches the principles of CBT. The study should be 8 weeks long and include 4 lessons for each of the 2 main principles of CBT.'},\n  {'from': 'gpt',\n   'value': \"Title: Renewing the Mind: A Bible Study on Cognitive Behavioral Therapy (CBT) Principles\\n\\nOverview: This eight-week Bible study aims to integrate the principles of Cognitive Behavioral Therapy (CBT) with biblical wisdom to help individuals renew their minds, transform their thinking, and experience positive change in their lives. Each week will focus on one of the key principles of CBT, emphasizing its connection to relevant biblical teachings and offering practical applications rooted in Christian faith.\\n\\nWeek 1: Introduction to Cognitive Behavioral Therapy (CBT)\\nLesson 1: Understanding the Mind-Body Connection\\n- Key CBT Principle: Thoughts, Emotions, and Behaviors Interact\\n- Biblical Perspective: Renewed Mind and a Transformed Life (Romans 12:2)\\n- Practical Applications: Identifying Automatic Negative Thoughts (ANTs)\\n\\nLesson 2: The Power of Perception\\n- Key CBT Principle: Challenging Cognitive Distortions\\n- Biblical Perspective: Taking Every Thought Captive (2 Corinthians 10:5)\\n- Practical Applications: Recognizing and Reframing Distorted Thinking\\n\\nWeek 2: Cognitive Restructuring and Biblical Truth\\nLesson 3: Identifying Core Beliefs and Assumptions\\n- Key CBT Principle: Examining Core Beliefs\\n- Biblical Perspective: Anchoring Beliefs in God's Word (Psalm 119:105)\\n- Practical Applications: Evaluating Core Beliefs\\n\\nLesson 4: Replacing Negative Thoughts\\n- Key CBT Principle: Cognitive Restructuring\\n- Biblical Perspective: Meditating on God's Truth (Psalm 19:14)\\n- Practical Applications: Using Scripture to Counter Negative Thoughts\\n\\nWeek 3: Addressing Emotions and the Role of Faith\\nLesson 5: Emotion Regulation\\n- Key CBT Principle: Recognizing and Managing Emotional Responses\\n- Biblical Perspective: Embracing God's Peace (Philippians 4:6-7)\\n- Practical Applications: Implementing Healthy Coping Strategies\\n\\nLesson 6: Faith and Resilience\\n- Key CBT Principle: Building Cognitive Resilience\\n- Biblical Perspective: Trusting in God's Plan (Proverbs 3:5-6)\\n- Practical Applications: Strengthening Resilience through Faith\\n\\nWeek 4: Behavioral Activation and Christian Living\\nLesson 7: Goal Setting and Behavioral Activation\\n- Key CBT Principle: Taking Action for Change\\n- Biblical Perspective: Diligence and Perseverance (Galatians 6:9)\\n- Practical Applications: Setting SMART Goals\\n\\nLesson 8: Living Out Christian Principles\\n- Key CBT Principle: Integrating Christian Values into Actions\\n- Biblical Perspective: Fruit of the Spirit (Galatians 5:22-23)\\n- Practical Applications: Applying Christian Principles in Daily Life\\n\\nConclusion: Reviewing and Applying CBT Principles within a Christian Framework\\n- Summarizing the 8-week study and reflecting on personal growth\\n- Encouraging continued integration of CBT principles and Christian values\\n\\nNote: While this Bible study is designed to integrate the principles of CBT with Christian faith, it is essential to remember that it does not replace professional therapy. Participants should be encouraged to seek professional help when needed.\"},\n  {'from': 'human',\n   'value': 'Create a 1 page overview for each lesson that includes a hook, objective, bible verse and 2 discussion questions. Write these in a way that makes the participants want to come.'},\n  {'from': 'gpt',\n   'value': 'Week 1: Introduction to Cognitive Behavioral Therapy (CBT)\\n\\nLesson 1: Understanding the Mind-Body Connection\\n\\nHook: Discover the power within you to transform your thoughts, emotions, and behaviors, as we embark on a journey to renew our minds through the principles of Cognitive Behavioral Therapy (CBT).\\n\\nObjective: By the end of this lesson, participants will understand the interconnectedness of thoughts, emotions, and behaviors, and how renewing the mind leads to transformation.\\n\\nBible Verse: \"Do not conform to the pattern of this world, but be transformed by the renewing of your mind. Then you will be able to test and approve what God’s will is—his good, pleasing and perfect will.\" - Romans 12:2\\n\\nDiscussion Questions:\\n1. How do you think our thoughts, emotions, and behaviors influence each other?\\n2. In what areas of your life do you feel the need for transformation and renewal?\\n\\nLesson 2: The Power of Perception\\n\\nHook: Uncover the truth about your thoughts and learn how to challenge the distortions that hinder your growth and well-being through a biblical lens. Join us as we explore the power of perception and transform our thinking.\\n\\nObjective: By the end of this lesson, participants will be able to identify common cognitive distortions and understand the importance of challenging negative thought patterns.\\n\\nBible Verse: \"We demolish arguments and every pretension that sets itself up against the knowledge of God, and we take captive every thought to make it obedient to Christ.\" - 2 Corinthians 10:5\\n\\nDiscussion Questions:\\n1. How have negative thought patterns influenced your perception of yourself and others?\\n2. What are some strategies you can use to challenge and reframe negative thoughts?\\n\\nNote: Each lesson overview should be formatted into a one-page document with the hook, objective, Bible verse, and discussion questions.'},\n  {'from': 'human',\n   'value': 'Write a 4 question quiz to go with lesson 1. The questions should be application based and use scripture to support the answers.'},\n  {'from': 'gpt',\n   'value': 'Quiz for Lesson 1: Understanding the Mind-Body Connection\\n\\nQuestion 1:\\nHow does the Bible encourage us to renew our minds?\\nA) By conforming to the patterns of this world\\nB) By transforming our behaviors\\nC) By renewing our thoughts\\n\\nScripture Reference: Romans 12:2\\n\\nQuestion 2:\\nTrue or False: According to CBT, our thoughts, emotions, and behaviors are interconnected.\\nA) True\\nB) False\\n\\nScripture Reference: N/A\\n\\nQuestion 3:\\nWhich of the following statements best captures the relationship between our thoughts, emotions, and behaviors?\\nA) Our thoughts have no impact on our emotions and behaviors.\\nB) Our emotions have no impact on our thoughts and behaviors.\\nC) Our thoughts, emotions, and behaviors influence each other.\\n\\nScripture Reference: N/A\\n\\nQuestion 4:\\nWhat is one practical step you can take to begin renewing your mind?\\nA) Engaging in negative self-talk\\nB) Identifying automatic negative thoughts\\nC) Conforming to the patterns of this world\\n\\nScripture Reference: N/A\\n\\nNote: Please ensure that the quiz questions are utilized within the context of the overall lesson and that the correct answers are indicated. The scripture references provided are for support and reflection but may not have direct correlation to the quiz questions.'}],\n 'num_of_rounds': 3}"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"## Load Llama 3 8B","metadata":{}},{"cell_type":"code","source":"from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer\nfrom trl import setup_chat_format\nimport torch\n# from bitsandbytes import BitsAndBytesConfig\n# QLoRA config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n)\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    'meta-llama/Meta-Llama-3-8B',\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    attn_implementation='eager'\n)\ntokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3-8B')\nmodel, tokenizer = setup_chat_format(model, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T07:11:26.208253Z","iopub.execute_input":"2025-03-04T07:11:26.208569Z","iopub.status.idle":"2025-03-04T07:14:08.463693Z","shell.execute_reply.started":"2025-03-04T07:11:26.208534Z","shell.execute_reply":"2025-03-04T07:14:08.462959Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70e2492dc4724efc922541cce02b6451"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da3e894b3fe944fb810b6407298c85b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd5507321d50453e9ec9606778a5f385"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ad75dd6f45a49f2be14fbcf6ff0c924"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ec5b40fb8ba466b970576dae7167969"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"416f6aa0a4554d7c8d49887a3bd9c820"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7718fd2c472b4ad79b431c386f645dde"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1d653db16db4f3baddecc656ef4dea7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/177 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f60668e1e4347bcb8f7ae6e90c85795"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7807d762d62847a2b2b2588cbd2da4f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1822f081c1244590ab02dd860b0c1474"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"124efd7883964c0caf4778b430146cbf"}},"metadata":{}},{"name":"stderr","text":"The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\nThe new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from peft import (\n    LoraConfig,\n    PeftModel,\n    prepare_model_for_kbit_training,\n    get_peft_model,\n)\n# LoRA config\npeft_config = LoraConfig(\n    r=8,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n)\nmodel = get_peft_model(model, peft_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T07:53:07.080829Z","iopub.execute_input":"2025-03-04T07:53:07.081217Z","iopub.status.idle":"2025-03-04T07:53:07.524978Z","shell.execute_reply.started":"2025-03-04T07:53:07.081188Z","shell.execute_reply":"2025-03-04T07:53:07.524321Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"def formatting_prompts_func(example):\n    k=[]\n    for converse in example['conversations']:\n        k.append({'role':converse['from'], 'content':'assistant' if converse['value'] == 'gpt' else 'user'})\n    example['text'] = tokenizer.apply_chat_template(k, tokenize=False)\n    return example","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T07:29:30.499718Z","iopub.execute_input":"2025-03-04T07:29:30.500082Z","iopub.status.idle":"2025-03-04T07:29:30.505488Z","shell.execute_reply.started":"2025-03-04T07:29:30.500048Z","shell.execute_reply":"2025-03-04T07:29:30.504706Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"dataset = dataset.map(formatting_prompts_func, num_proc=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T07:29:30.865958Z","iopub.execute_input":"2025-03-04T07:29:30.866240Z","iopub.status.idle":"2025-03-04T07:29:31.520523Z","shell.execute_reply.started":"2025-03-04T07:29:30.866219Z","shell.execute_reply":"2025-03-04T07:29:31.519698Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3aa571cca6d467d89c94fd81f517016"}},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"from transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\ntraining_arguments = TrainingArguments(\n    output_dir='./models/',\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=2,\n    optim=\"paged_adamw_32bit\",\n    num_train_epochs=1,\n    evaluation_strategy=\"steps\",\n    eval_steps=0.2,\n    logging_steps=1,\n    warmup_steps=10,\n    logging_strategy=\"steps\",\n    learning_rate=2e-4,\n    fp16=False,\n    bf16=False,\n    group_by_length=True,\n    report_to=\"wandb\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T07:30:21.843169Z","iopub.execute_input":"2025-03-04T07:30:21.843477Z","iopub.status.idle":"2025-03-04T07:30:21.879952Z","shell.execute_reply.started":"2025-03-04T07:30:21.843453Z","shell.execute_reply":"2025-03-04T07:30:21.879061Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"from trl import SFTTrainer, SFTConfig\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    args = SFTConfig(\n        dataset_text_field=\"text\",\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=2,\n        warmup_steps = 5,\n        num_train_epochs = 1, # Set this for 1 full training run.\n        #max_steps = 60,\n        learning_rate = 2e-4,\n        fp16 = False,\n        bf16 = True,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"model_traning_outputs\",\n        report_to = \"wandb\",\n        max_seq_length = 512,\n        dataset_num_proc = 4,\n        packing = False, # Can make training 5x faster for short sequences.\n    ),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T07:56:30.727699Z","iopub.execute_input":"2025-03-04T07:56:30.728177Z","iopub.status.idle":"2025-03-04T07:56:31.157767Z","shell.execute_reply.started":"2025-03-04T07:56:30.728134Z","shell.execute_reply":"2025-03-04T07:56:31.156853Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-35-f2b93e31e42d>:2: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n  trainer = SFTTrainer(\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T07:56:37.305703Z","iopub.execute_input":"2025-03-04T07:56:37.306068Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='68' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 68/250 24:03 < 1:06:19, 0.05 it/s, Epoch 0.27/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"%export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T07:54:59.016899Z","iopub.execute_input":"2025-03-04T07:54:59.017267Z","iopub.status.idle":"2025-03-04T07:54:59.205411Z","shell.execute_reply.started":"2025-03-04T07:54:59.017240Z","shell.execute_reply":"2025-03-04T07:54:59.203867Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}